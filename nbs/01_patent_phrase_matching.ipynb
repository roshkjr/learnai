{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1e5a53-dfd7-4c64-87e4-35ab9042c13b",
   "metadata": {},
   "source": [
    "## patent\n",
    "> Module for training patent phrase matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df3d7f-f725-4ea0-a3b1-a6193466f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp patent_phrase_matching.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd18dc1-51b0-4ee5-9f76-64777cc1dfca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|export\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ffefe-1fb7-409f-b2a0-d6ed537949e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "import kaggle\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e42af6b-4c07-4a63-8279-f056441a3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "path = Path(f'{os.getenv(\"DATA_BASE_DIR\")}/us-patent-phrase-to-phrase-matching')\n",
    "!ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cb490-0a9b-4b1a-a6f3-00caef0df757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#!kaggle competitions download us-patent-phrase-to-phrase-matching -p {path}\n",
    "# zipfile.ZipFile(f'{path}/{Path(\"us-patent-phrase-to-phrase-matching\")}.zip').extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995f0e50-a5ac-44f5-8c8b-12fbed5c7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caa4049-13c1-406e-9e50-abdd8ea9f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "df = pd.read_csv(f\"{path}/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30968c-a61b-48ae-989b-57f3c34e1236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     8d135da0b55b8c88  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f723f-dd44-47a3-94ad-df623b1ee432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "df[\"score\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651e989-3e5b-44d3-9d34-59727aeaf192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.75, 0.25, 0.  , 1.  ])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "df[\"score\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9b0cb-d681-48bc-b45c-fb20501afb5e",
   "metadata": {},
   "source": [
    "The scores are values between 0 and 1, hence this can be modeled as a single class prediction problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf9656-c036-422f-957b-3e92285d8505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import argparse\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from transformers import HfArgumentParser\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fecd9-3dbc-4734-b038-649a3bbac222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hps/software/users/pdbe/roshan/llm/llm_venv/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "model_ckpt = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt, cache_dir=os.getenv(\"HF_HUB_CACHE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef343b-ef43-4893-899d-df43e7d6c15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7f9c8-b45b-45b4-83d8-eb64add2e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "df[\"text\"] = df[\"anchor\"] + tokenizer.sep_token + df['target'] + tokenizer.sep_token + df['context']\n",
    "df.rename(columns={\"score\":\"label\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e073c02-b043-42f1-91d7-862c880e01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "anchors = df.anchor.unique()\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfbeb0-2577-4e24-91bf-f72e38ec9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "val_prop = 0.25\n",
    "val_sz = int(len(anchors)*val_prop)\n",
    "val_anchors = anchors[:val_sz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d8610-ffdc-4cb9-b328-bb9778851a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "train_df = df[~df['anchor'].isin(val_anchors)]\n",
    "val_df = df[df['anchor'].isin(val_anchors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b025a13-8c9b-4c31-b30d-62ea4c7738ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27357, 6), (9116, 6))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd5919-63c3-4a0b-866a-04b273c434aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3623021530138539, 0.3613426941641071)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "train_df['label'].mean(), val_df['label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d1d39-b855-416f-8680-dead9bbafeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a8b35-3790-441a-9a54-d0b95174b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "test_data = pd.read_csv(f\"{path}/test.csv\")\n",
    "test_data[\"text\"] = test_data[\"anchor\"] + tokenizer.sep_token + test_data['target'] + tokenizer.sep_token + test_data['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc6ce8-5171-474c-82d2-48eecaac9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "test_ds = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a95b6-af51-40e6-93ff-1fd77c19b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "patents = DatasetDict({\"train\": train_ds,\n",
    "                       \"validation\":val_ds,\n",
    "                       \"test\": test_ds\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35620beb-e1eb-46b2-998f-ff13ce8da83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'anchor': Value(dtype='string', id=None),\n",
       " 'target': Value(dtype='string', id=None),\n",
       " 'context': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='float64', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "patents['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f679b-873e-4453-8657-837e437ab42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /homes/roshan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mroshkjr\u001b[0m (\u001b[33mroshkjr-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf169a7-e25e-41b9-9cca-05c119d43b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=\"us-patent-phrase-to-phrase-matching\"\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "%env WANDB_PROJECT=\"us-patent-phrase-to-phrase-matching\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a07079-89c8-42e5-8077-823305b33258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "patents_encoded.set_format(\"torch\") #setting the format to torch so that we can use to(dvice) of torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb118e-f2a8-4f47-803f-1d26e2dd2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "def get_output(batch):\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48efe337-6372-452c-a239-84a5ba51cf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "from omegaconf import OmegaConf\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a3a24-271e-48a2-8624-4650ac9c1bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "with open(\"../configs/patent_phrase_matching/config.json\", 'r') as fh:\n",
    "    conf = OmegaConf.create(json.load(fh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc31f92-4fa6-4d03-acdb-d0361b016916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per_device_train_batch_size: 8\n",
      "per_device_eval_batch_size: 8\n",
      "wandb_project: patent_phrase_matching\n",
      "wandb_job_type: Seq2Class\n",
      "lr_scheduler_type: cosine\n",
      "eval_strategy: epoch\n",
      "model_name: microsoft/deberta-v3-small\n",
      "dtype: float32\n",
      "text_column: text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "print(OmegaConf.to_yaml(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb44b07-a9de-40bb-a9c1-c9f6a610c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_metrics(pred):\n",
    "    return {'pearson': np.corrcoef(*pred)[0][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614be38-d686-4c43-8a4f-b40c0a27038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class HfModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: Optional[str] = field(\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization. \"\n",
    "            \"Don't set if you want to train a model from scratch. \"\n",
    "            \"W&B artifact references are supported in addition to the sources supported by `PreTrainedModel`.\"\n",
    "        },\n",
    "    )\n",
    "    num_labels: int = field(\n",
    "        metadata={\"help\": \"Number of labels to classify\"},\n",
    "    )\n",
    "    dropout: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Dropout rate. Overwrites config.\"},\n",
    "    )\n",
    "    activation_dropout: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Activation dropout rate. Overwrites config.\"},\n",
    "    )\n",
    "    attention_dropout: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Attention dropout rate. Overwrites config.\"},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961cc5c2-858c-4915-bf8e-1ab08b87009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class HfDataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    text_column: Optional[str] = field(\n",
    "        metadata={\n",
    "            \"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"\n",
    "        },\n",
    "    )\n",
    "    filter_column: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Column that containts classes to be filtered.\"},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The number of processes to use for the preprocessing. Not used in streaming mode.\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416494ca-8cdc-4541-b5ab-fac786491b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class HfTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to training parameters.\n",
    "    \"\"\"\n",
    "    output_dir: str = field(\n",
    "        metadata={\n",
    "            \"help\": \"The output directory where the model predictions and checkpoints will be written.\"\n",
    "        },\n",
    "    )\n",
    "    batch_size: int = field(\n",
    "        metadata={\n",
    "            \"help\": \"The size of batch\"\n",
    "        },\n",
    "    )\n",
    "    epochs: int = field(\n",
    "        metadata={\n",
    "            \"help\": \"The number of epochs to run\"\n",
    "        },\n",
    "    )\n",
    "    warmup_ratio: float = field(\n",
    "        metadata={\"help\":\"Warmup ratio to use\"}\n",
    "    )\n",
    "    optimizer: str = field(\n",
    "        metadata={\n",
    "            \"help\": 'The optimizer to use. Can be \"adam\" or \"adafactor\"'\n",
    "        },\n",
    "    )\n",
    "    eval_strategy: str = field(\n",
    "        metadata={\"help\": 'The srategy for evaluation'}\n",
    "    )\n",
    "    weight_decay: float = field(\n",
    "        metadata={\"help\": \"Weight decay applied to parameters.\"}\n",
    "    )\n",
    "    num_train_epochs: int = field(\n",
    "        metadata={\"help\": \"Total number of training epochs to perform.\"}\n",
    "    )\n",
    "    per_device_train_batch_size: int = field(\n",
    "        metadata={\"help\": \"Batch size per data parallel device for training.\"},\n",
    "    )\n",
    "    per_device_eval_batch_size: Optional[int] = field(\n",
    "        metadata={\n",
    "            \"help\": \"Batch size per data parallel device for evaluation. Same as training batch size if not set.\"\n",
    "        },\n",
    "    )\n",
    "    learning_rate: float = field(\n",
    "        metadata={\"help\": \"The initial learning rate.\"}\n",
    "    )\n",
    "    lr_scheduler_type: str = field(\n",
    "        metadata={\"help\":\"The learning rate scheduler type\"}\n",
    "    )\n",
    "    wandb_project: str = field(\n",
    "        metadata={\"help\": \"The name of the wandb project.\"},\n",
    "    )\n",
    "    wandb_job_type: str = field(\n",
    "        metadata={\"help\": \"The name of the wandb job type.\"},\n",
    "    )\n",
    "    overwrite_output_dir: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Overwrite the content of the output directory. \"\n",
    "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n",
    "    do_eval: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to run eval on the validation set.\"}\n",
    "    )\n",
    "    seed_model: int = field(\n",
    "        default=42,\n",
    "        metadata={\n",
    "            \"help\": \"Random seed for the model that will be set at the beginning of training.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.optimizer in [\n",
    "            \"adam\",\n",
    "            \"adafactor\",\n",
    "        ], f\"Selected optimizer not supported: {self.optim}\"\n",
    "        if self.optimizer == \"adafactor\" and self.weight_decay == 0:\n",
    "            self.weight_decay = None\n",
    "        if self.per_device_eval_batch_size is None:\n",
    "            self.per_device_eval_batch_size = self.per_device_train_batch_size\n",
    "        if not self.do_train:\n",
    "            self.num_train_epochs = 1\n",
    "        if (\n",
    "            os.path.exists(self.output_dir)\n",
    "            and os.listdir(self.output_dir)\n",
    "            and self.do_train\n",
    "            and not self.overwrite_output_dir\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({self.output_dir}) already exists and is not empty.\"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bc877-2bcf-4e69-9152-2a7ce89c1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog=\"patent_phrase_matching\",\n",
    "        description='train patent_phrase_matching',\n",
    "    )\n",
    "    parser.add_argument('--config',\n",
    "                       default=\"../configs/patent_phrase_matching/config.json\")\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25051cd-d689-49d2-b247-b1c100dcb162",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize(batch, tokenizer, data_args):\n",
    "    return tokenizer(batch[data_args.text_column], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7959fc2-11fd-4464-a939-a07068f8a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dds(df, separator, train_idx, val_idx, tokenizer, text_column):\n",
    "    df[\"text\"] = df[\"anchor\"] + separator + df['target'] + separator + df['context']\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds_encoded = ds.map(partial(tokenize, tokenizer=tokenizer, text_column=text_column), batched=True)\n",
    "    return DatasetDict({\"train\":ds_encoded.select(train_idx), \"val\": ds_encoded.select(val_idx)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dace04-7ad4-4e59-b610-3b7407e9a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main():\n",
    "    hf_parser = HfArgumentParser(\n",
    "        (HfModelArguments, HfDataTrainingArguments, HfTrainingArguments)\n",
    "    )\n",
    "    parser = create_parser()\n",
    "    args = parser.parse_args(['--config', '../configs/patent_phrase_matching/config.json'])\n",
    "    model_args, data_args, train_args = hf_parser.parse_json_file(\n",
    "        json_file=args.config\n",
    "    )\n",
    "    \n",
    "    path = Path(f'{os.getenv(\"DATA_BASE_DIR\")}/us-patent-phrase-to-phrase-matching')\n",
    "    df = pd.read_csv(f\"{path}/train.csv\").rename(columns={\"score\": \"label\"})\n",
    "    \n",
    "    anchors = df.anchor.unique()\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(anchors)\n",
    "    \n",
    "    val_prop = 0.25\n",
    "    val_sz = int(len(anchors) * val_prop)\n",
    "    val_anchors = anchors[:val_sz]\n",
    "    is_val = np.isin(df.anchor, val_anchors)\n",
    "    idxs = np.arange(len(df))\n",
    "    val_idxs = idxs[is_val]\n",
    "    trn_idxs = idxs[~is_val]\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name,\n",
    "        cache_dir=os.getenv(\"HF_HUB_CACHE\")\n",
    "    )\n",
    "    separator = tokenizer.sep_token\n",
    "    dds = get_dds(df, separator, trn_idxs, val_idxs, tokenizer, data_args.text_column)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = (\n",
    "        AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name,\n",
    "            num_labels=model_args.num_labels,\n",
    "            cache_dir=os.getenv(\"HF_HUB_CACHE\")\n",
    "        )\n",
    "    ).to(device)\n",
    "    \n",
    "    logging_steps = len(dds[\"train\"]) // train_args.batch_size\n",
    "    model_name = f\"{model_args.model_name}-finetuned-patents\"\n",
    "    output_dir = f\"{os.getenv('MODEL_BASE_DIR')}/model_name\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=train_args.output_dir, \n",
    "        num_train_epochs=train_args.epochs,\n",
    "        learning_rate=train_args.learning_rate,\n",
    "        warmup_ratio=train_args.warmup_ratio,\n",
    "        lr_scheduler_type=train_args.lr_scheduler_type,\n",
    "        fp16=True,\n",
    "        per_device_train_batch_size=train_args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=train_args.per_device_eval_batch_size,\n",
    "        weight_decay=train_args.weight_decay, \n",
    "        eval_strategy=train_args.eval_strategy,\n",
    "        disable_tqdm=False,\n",
    "        logging_steps=logging_steps,\n",
    "        push_to_hub=False,\n",
    "        log_level=\"error\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dds[\"train\"],\n",
    "        eval_dataset=dds[\"val\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "        processing_class=tokenizer\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
