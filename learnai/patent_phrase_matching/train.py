# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01_patent_phrase_matching.ipynb.

# %% auto 0
__all__ = ['compute_metrics', 'HfModelArguments', 'HfDataTrainingArguments', 'HfTrainingArguments', 'create_parser', 'tokenize',
           'get_dds', 'main']

# %% ../../nbs/01_patent_phrase_matching.ipynb 2
from pathlib import Path
import os
from dotenv import load_dotenv
load_dotenv()

# %% ../../nbs/01_patent_phrase_matching.ipynb 6
import pandas as pd
from datasets import Dataset, DatasetDict

# %% ../../nbs/01_patent_phrase_matching.ipynb 12
import os
import torch
import wandb
import argparse
import numpy as np
from functools import partial
from typing import Optional
from dataclasses import asdict, dataclass, field
from transformers import HfArgumentParser
from transformers import Trainer, TrainingArguments
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# %% ../../nbs/01_patent_phrase_matching.ipynb 33
def compute_metrics(pred):
    return {'pearson': np.corrcoef(*pred)[0][1]}

# %% ../../nbs/01_patent_phrase_matching.ipynb 34
@dataclass
class HfModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.
    """

    model_name: Optional[str] = field(
        metadata={
            "help": "The model checkpoint for weights initialization. "
            "Don't set if you want to train a model from scratch. "
            "W&B artifact references are supported in addition to the sources supported by `PreTrainedModel`."
        },
    )
    num_labels: int = field(
        metadata={"help": "Number of labels to classify"},
    )
    dropout: Optional[float] = field(
        default=None,
        metadata={"help": "Dropout rate. Overwrites config."},
    )
    activation_dropout: Optional[float] = field(
        default=None,
        metadata={"help": "Activation dropout rate. Overwrites config."},
    )
    attention_dropout: Optional[float] = field(
        default=None,
        metadata={"help": "Attention dropout rate. Overwrites config."},
    )


# %% ../../nbs/01_patent_phrase_matching.ipynb 35
@dataclass
class HfDataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """

    text_column: Optional[str] = field(
        metadata={
            "help": "The name of the column in the datasets containing the full texts (for summarization)."
        },
    )
    filter_column: Optional[str] = field(
        default=None,
        metadata={"help": "Column that containts classes to be filtered."},
    )
    max_train_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of training examples."
        },
    )
    max_eval_samples: Optional[int] = field(
        default=None,
        metadata={
            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples."
        },
    )
    preprocessing_num_workers: Optional[int] = field(
        default=None,
        metadata={
            "help": "The number of processes to use for the preprocessing. Not used in streaming mode."
        },
    )

# %% ../../nbs/01_patent_phrase_matching.ipynb 36
@dataclass
class HfTrainingArguments:
    """
    Arguments pertaining to training parameters.
    """
    output_dir: str = field(
        metadata={
            "help": "The output directory where the model predictions and checkpoints will be written."
        },
    )
    batch_size: int = field(
        metadata={
            "help": "The size of batch"
        },
    )
    epochs: int = field(
        metadata={
            "help": "The number of epochs to run"
        },
    )
    warmup_ratio: float = field(
        metadata={"help":"Warmup ratio to use"}
    )
    optimizer: str = field(
        metadata={
            "help": 'The optimizer to use. Can be "adam" or "adafactor"'
        },
    )
    eval_strategy: str = field(
        metadata={"help": 'The srategy for evaluation'}
    )
    weight_decay: float = field(
        metadata={"help": "Weight decay applied to parameters."}
    )
    num_train_epochs: int = field(
        metadata={"help": "Total number of training epochs to perform."}
    )
    per_device_train_batch_size: int = field(
        metadata={"help": "Batch size per data parallel device for training."},
    )
    per_device_eval_batch_size: Optional[int] = field(
        metadata={
            "help": "Batch size per data parallel device for evaluation. Same as training batch size if not set."
        },
    )
    learning_rate: float = field(
        metadata={"help": "The initial learning rate."}
    )
    lr_scheduler_type: str = field(
        metadata={"help":"The learning rate scheduler type"}
    )
    wandb_project: str = field(
        metadata={"help": "The name of the wandb project."},
    )
    wandb_job_type: str = field(
        metadata={"help": "The name of the wandb job type."},
    )
    overwrite_output_dir: bool = field(
        default=False,
        metadata={
            "help": (
                "Overwrite the content of the output directory. "
                "Use this to continue training if output_dir points to a checkpoint directory."
            )
        },
    )
    do_train: bool = field(default=False, metadata={"help": "Whether to run training."})
    do_eval: bool = field(
        default=False, metadata={"help": "Whether to run eval on the validation set."}
    )
    seed_model: int = field(
        default=42,
        metadata={
            "help": "Random seed for the model that will be set at the beginning of training."
        },
    )

    def __post_init__(self):
        assert self.optimizer in [
            "adam",
            "adafactor",
        ], f"Selected optimizer not supported: {self.optim}"
        if self.optimizer == "adafactor" and self.weight_decay == 0:
            self.weight_decay = None
        if self.per_device_eval_batch_size is None:
            self.per_device_eval_batch_size = self.per_device_train_batch_size
        if not self.do_train:
            self.num_train_epochs = 1
        if (
            os.path.exists(self.output_dir)
            and os.listdir(self.output_dir)
            and self.do_train
            and not self.overwrite_output_dir
        ):
            raise ValueError(
                f"Output directory ({self.output_dir}) already exists and is not empty."
                "Use --overwrite_output_dir to overcome."
            )



# %% ../../nbs/01_patent_phrase_matching.ipynb 37
def create_parser():
    parser = argparse.ArgumentParser(
        prog="patent_phrase_matching",
        description='train patent_phrase_matching',
    )
    parser.add_argument('--config',
                       default="../configs/patent_phrase_matching/config.json")
    return parser

# %% ../../nbs/01_patent_phrase_matching.ipynb 38
def tokenize(batch, tokenizer, data_args):
    return tokenizer(batch[data_args.text_column], padding=True, truncation=True)

# %% ../../nbs/01_patent_phrase_matching.ipynb 39
def get_dds(df, separator, train_idx, val_idx, tokenizer, text_column):
    df["text"] = df["anchor"] + separator + df['target'] + separator + df['context']
    ds = Dataset.from_pandas(df)
    ds_encoded = ds.map(partial(tokenize, tokenizer=tokenizer, text_column=text_column), batched=True)
    return DatasetDict({"train":ds_encoded.select(train_idx), "val": ds_encoded.select(val_idx)})

# %% ../../nbs/01_patent_phrase_matching.ipynb 40
def main():
    hf_parser = HfArgumentParser(
        (HfModelArguments, HfDataTrainingArguments, HfTrainingArguments)
    )
    parser = create_parser()
    args = parser.parse_args(['--config', '../configs/patent_phrase_matching/config.json'])
    model_args, data_args, train_args = hf_parser.parse_json_file(
        json_file=args.config
    )
    
    path = Path(f'{os.getenv("DATA_BASE_DIR")}/us-patent-phrase-to-phrase-matching')
    df = pd.read_csv(f"{path}/train.csv").rename(columns={"score": "label"})
    
    anchors = df.anchor.unique()
    np.random.seed(42)
    np.random.shuffle(anchors)
    
    val_prop = 0.25
    val_sz = int(len(anchors) * val_prop)
    val_anchors = anchors[:val_sz]
    is_val = np.isin(df.anchor, val_anchors)
    idxs = np.arange(len(df))
    val_idxs = idxs[is_val]
    trn_idxs = idxs[~is_val]
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_args.model_name,
        cache_dir=os.getenv("HF_HUB_CACHE")
    )
    separator = tokenizer.sep_token
    dds = get_dds(df, separator, trn_idxs, val_idxs, tokenizer, data_args.text_column)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = (
        AutoModelForSequenceClassification.from_pretrained(
            model_args.model_name,
            num_labels=model_args.num_labels,
            cache_dir=os.getenv("HF_HUB_CACHE")
        )
    ).to(device)
    
    logging_steps = len(dds["train"]) // train_args.batch_size
    model_name = f"{model_args.model_name}-finetuned-patents"
    output_dir = f"{os.getenv('MODEL_BASE_DIR')}/model_name"
    
    training_args = TrainingArguments(
        output_dir=train_args.output_dir, 
        num_train_epochs=train_args.epochs,
        learning_rate=train_args.learning_rate,
        warmup_ratio=train_args.warmup_ratio,
        lr_scheduler_type=train_args.lr_scheduler_type,
        fp16=True,
        per_device_train_batch_size=train_args.per_device_train_batch_size,
        per_device_eval_batch_size=train_args.per_device_eval_batch_size,
        weight_decay=train_args.weight_decay, 
        eval_strategy=train_args.eval_strategy,
        disable_tqdm=False,
        logging_steps=logging_steps,
        push_to_hub=False,
        log_level="error"
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dds["train"],
        eval_dataset=dds["val"],
        compute_metrics=compute_metrics,
        processing_class=tokenizer
    )
    
    trainer.train()

